# Reference
Please refer to the [course main page](https://www.coursera.org/learn/probabilistic-models-in-nlp/home/welcome) of the coursera for more code. 
[**POS**](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html): this is the table that explains meaning of each POS tag.
***Book***: [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/), [Dan Jurafsky](http://web.stanford.edu/people/jurafsky/) and [James H. Martin](http://www.cs.colorado.edu/~martin/) 

C2_W1 ref: the auto-correct you are about to implement was first created by [Peter Norvig](https://en.wikipedia.org/wiki/Peter_Norvig) in 2007. His [original article](https://norvig.com/spell-correct.html) may be a useful reference for the weekly assignment.

C2_W2 ref: another POS tag table with additional phrases tags can be found in at [Penn Treebank II tag set](https://www.clips.uantwerpen.be/clips.bak/pages/mbsp-tags) that gives you a complete description.



# Outline
## C2_W1: Autocorrect and Minimum Edit Distance
1. Autocorrect
2. Building the model
3. Lab: Building the vocabulary
4. Building the model II
5. Lab: Candidates from edits
6. Minimum edit distance
7. Minimum edit distance algorithm
8. Minimum edit distance algorithm II
9. Minimum edit distance algorithm III
- Programming Assignment: Autocorrect

## C2_W2: Part of Speech Tagging
1. Lecture: Part of Speech Tagging
2. Lab: Working with text files
3. Markov Chains
4. Markov Chains and POS Tags
5. Hidden Markov Models
6. Calculating Probabilities
7. Populating the Transition Matrix
8. Populating the Emission Matrix
9. Lab: Working with tags and Numpy
10. The Viterbi Algorithm
11. Viterbi Initialization
12. Viterbi: Forward Pass
13. Reading: Viterbi: Backward Pass
- Programming Assignment: Part of Speech Tagging

## C2_W3: Autocomplete
1. N-Grams Overview
1. N-grams and Probabilities
1. Sequence Probabilities
1. Starting and Ending Sentences
1. Lab: Corpus preprocessing for N-grams
1. The N-gram Language Model
1. Lab: Building the language model
1. Language Model Evaluation
1. Out of Vocabulary Words
1. Smoothing
1. Lab: Language model generalization

- Programming Assignment: Autocomplete

## C2_W4: Word Embeddings
1. Basic Word Representations
1. Word Embeddings
1. How to Create Word Embeddings?
1. Word Embedding Methods
1. Continuous Bag of Words Model
1. Cleaning and Tokenization
1. Sliding Window of words in Python
1. Transforming Words into Vectors
1. Lab: Data Preparation
1. Architecture for the CBOW Model
1. Architecture of the CBOW Model: Dimensions
1. Architecture of the CBOW Model: Dimensions
1. Architecture of the CBOW Model: Activation Functions
1. Lab: Intro to CBOW model
1. Training a CBOW Model: Cost Function
1. Training a CBOW Model: Forward Propagation
1. Training a CBOW Model: Backpropagation and Gradient Descent
1. Lab: Training the CBOW model
1. Extracting Word Embedding Vectors
1. Lab: Word Embeddings
1. Evaluating Word Embeddings: Intrinsic Evaluation
1. Evaluating Word Embeddings: Extrinsic Evaluation
1. Lab: Lecture notebook: Word embeddings step by step

- Programming Assignment: word embeddings