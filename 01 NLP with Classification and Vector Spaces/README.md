# Reference
Please refer to the [course main page](https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/welcome) of the coursera for more code. 

# Outline
## C1_W1: Sentment Analysis with Logistic Regression 
1. Supervised ML & Sentiment Analysis
2. Vocabulary & Feature Extraction
3. Negative and Positive Frequencies
4. Feature Extraction with Frequencies 
5. Preprocessing
6. Lab: Natural Language preprocessing
7. Putting it all together
8. Visualizing word frequencies
9. Logistic Regression Overview
10. Logistic Regression: Training
11. Lab: Visualizing tweets and Logistic Regression models
12. Logistic Regression: Testing
13. Reading: Optional Logistic Regression: Cost Function
14. Reading: Optional Logistic Regression: Gradient
- Programming Assignment: Logistic Regression 

## C1_W2: Sentment Analysis with Naïve Bayes 
1. Probability and Bayes’ Rule
2. Bayes' Rule
3. Naive Bayes Introduction
4. **Laplacian Smoothing**
5. Log Likelihood, Part 1
6. Log Likelihood Part 2
7. Training naïve Bayes
8. Lab: Visualizing likelihoods and confidence ellipses
9. Testing naïve Bayes
10. Applications of Naive Bayes
11. Naïve Bayes Assumptions
12. Error Analysis
- Programming Assignment: Naïve Bayes

## C1_W3: Vector Space Models 
1. Vector Space Models
2. Word by Word and Word by Doc.
3. Lab: Linear algebra in Python with Numpy
4. Euclidian Distance
5. Cosine Similarity: Intuition
6. Cosine Similarity
7. Manipulating Words in Vector Spaces
8. Lab: Manipulating word embeddings
9. Visualization and PCA
10. PCA algorithm
11. Lab: Another explanation about PCA
- Programming Assignment: Vector Space Models and PCA

## C1_W4: Machine Translation and Document Search 
1. Transforming word vectors
2. Lab: Rotation matrices in R2
3. K-nearest neighbors
4. **Hash tables and hash functions**
5. **Locality sensitive hashing**
6. Multiple Planes
7. **Lab: Hash tables**
8. **Approximate nearest neighbors**
9. Searching documents
- Programming Assignment: Word Translation 