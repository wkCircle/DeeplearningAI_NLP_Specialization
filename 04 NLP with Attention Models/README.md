# Reference
Please refer to the [course main page](https://www.coursera.org/learn/attention-models-in-nlp/home/welcome) of the coursera for more code. 

# Outline
## C4_W1: Neural Machine Translation
1. Seq2seq
2. Seq2seq Model with Attention
3. Background on seq2seq
4. Queries, Keys, Values, and Attention
5. Lab: Ungraded Lab: Basic Attention
6. Setup for Machine Translation
7. Lab: Scaled Dot-Product Attention
8. Teacher Forcing
9. NMT Model with Attention
10. BLEU Score
11. Lab: BLEU Score
12. ROUGE-N Score
13. **Sampling and Decoding**: Greedy Decoding, Random Sampling, and Temperature method.
14. Beam Search
15. Minimum Bayes Risk
16. Lab: Stack Semantics
17. Week Conclusion
18. Reading: Content Resource
- Programming Assignment: NMT with Attention

## C4_W2: Text Summarization
1. Text Summarization
2. Transformers vs RNNs
3. Transformers overview
4. Transformer Applications
5. Scaled and Dot-Product Attention
6. Masked Self Attention
7. Multi-head Attention
8. Lab: Attention
9. Transformer Decoder
10. Transformer Summarizer
11. Lab: The Transformer Decoder
- Programming Assignment: Transformer Summarizer

## C4_W3: Question Answering
1. Transfer Learning in NLP
2. ELMo, GPT, BERT, T5
3. Bidirectional Encoder Representations from Transformers (BERT)
4. BERT Objective
5. Fine tuning BERT
6. Transformer T5
7. Multi-Task Training Strategy
8. GLUE Benchmark
9. Question Answering
10. Lab: SentencePiece and BPE
11. Welcome to Hugging Face ðŸ¤—
12. Hugging Face Introduction
13. Hugging Face I
14. Hugging Face II
15. Hugging Face III
16. Lab: Question Answering with HuggingFace 1
17. Lab: Question Answering with HuggingFace 2
- Programming Assignment: Question Answering

## C4_W4: Chatbot
1. Tasks with Long Sequences
2. Optional AI Storytelling
3. Transformer Complexity
4. LSH Attention
5. Optional KNN & LSH Review
6. Lab: Reformer LSH
7. Motivation for Reversible Layers: Memory!
8. Reversible Residual Layers
9. Lab: Revnet
10. Reformer
11. Optional Transformers beyond NLP

- Programming Assignment: Chatbot
